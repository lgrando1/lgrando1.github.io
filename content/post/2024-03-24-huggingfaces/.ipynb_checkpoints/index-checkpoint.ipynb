{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "title: Utilizando a plataforma Hugging Face para testar modelos de aprendizado de máquina. \n",
    "author: '' \n",
    "date: '2024-03-24' \n",
    "slug: hface\n",
    "categories: [] \n",
    "tags: [ \"Hugging Face\", \"IA\", \"ML\", \"Generativos\", \"Python\"] \n",
    "subtitle: 'Como testar modelos do Hugging Face em tarefas de Aprendizado de Máquinas' \n",
    "Summary: 'Criando um pipeline para testar os modelos presentes no Hugging Faces' \n",
    "authors: [] \n",
    "lastmod: '2024-03-24T16:00:00-03:00' \n",
    "featured: yes \n",
    "image:\n",
    "  caption: ''\n",
    "  focal_point: ''\n",
    "  preview_only: no\n",
    "projects: []\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plataforma [**Hugging Face**](https://huggingface.co/) é uma portal onde a comunidade de aprendizado de máquina colabora com modelos, conjunto de dados e aplicações.\n",
    "\n",
    "Ao acessar o site e clicar no link [Models](https://huggingface.co/models) é possível buscar por variados modelos voltados para várias tarefas de aprendizado de máquina visão computacional, processamento natural de linguagem, áudio, dados tabulares, aprendizado por reforço e outros tipos.\n",
    "\n",
    "Neste post apresentaremos uma introdução de como utilizar estas bibliotecas em sua máquina (ou no Google Colab). Como exemplo é demostrado a realização de duas tarefas: o preenchimento de mascaras de texto (completar um espaço de um texto) e o resumo de um texto. \n",
    "\n",
    "São dois modelos/exemplos simples, mas o objetivo é realmente despertar a curiosidade em conhecer mais sobre esta plataforma.\n",
    "\n",
    "Algumas considerações: \n",
    "1. Ao baixar o modelo em sua maquina, alguns modelos são grandes, como o segundo modelo deste tutorial que possui mais do que 1,5 GB. Neste [link](https://huggingface.co/docs/huggingface_hub/en/guides/manage-cache) é possível ver como gerenciar o cache destes modelos;\n",
    "2. Se atente ao modelo que você vai testar, pois já foram encontrados [problemas de segurança](https://www.bleepingcomputer.com/news/security/malicious-ai-models-on-hugging-face-backdoor-users-machines/);\n",
    "3. Se atente também nas licenças de conteúdo dos modelos e também possíveis dependências. Se atente a documentação presente em cada página dos modelos;\n",
    "4. Alguns modelos de aprendizados de máquinas exigem bastante recursos computacionais, ao escrever este post, várias vezes o Jupyter acabou resetando. Apenas para comparativo, este computador é um Core i5 de nona geração (Intel i5 - 9300H) e 8 GB de RAM. Infelizmente ainda não consegui ativar a GPU para tarefas de Machine Learning no Linux. No Google Colab é possível ativar o [suporte ao GPU](https://colab.research.google.com/notebooks/gpu.ipynb) mesmo no tier grátis.        \n",
    "\n",
    "Alertas feitos, vamos aos modelos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idNlGm1mrkCQ"
   },
   "source": [
    "Primeiro é necessário a biblioteca [Transformers](https://huggingface.co/docs/transformers) para poder baixar e treinais os modelos pré treinados. \n",
    "\n",
    "No momento da escrita deste post estão disponíveis 564772 modelos.\n",
    "\n",
    "Aqui esta presente a [documentação](https://huggingface.co/docs/transformers/en/installation) de como instalar esta biblioteca.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-3uqIu0-sC2s"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "#Apenas para suprimir erros, não nescessário. \n",
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDd-wLO7sM5R"
   },
   "source": [
    "## Tarefa 1 - preenchimento de mascaras\n",
    "\n",
    "Para realizar a tarefa de **preenchimento de mascaras**, utilizaremos o modelo [BERTimbau Base (aka \"bert-base-portuguese-cased\"](https://huggingface.co/neuralmind/bert-base-portuguese-cased) [1] \n",
    "\n",
    "\n",
    "Iremos utilizar neste caso a versão base. \n",
    "\n",
    "A tarefa realizada será \"fill-mask\" e iremos pedir que ele devolva 5 respostas para a frase \"Batatinha quando nasce, esparrama pelo [MASK]\" onde [MASK] é o texto que será preenchido pelo token.\n",
    "\n",
    "[1] SOUZA, Fábio e NOGUEIRA, Rodrigo e LOTUFO, **Roberto. BERTimbau: pretrained BERT models for Brazilian Portuguese.** 2020, [S.l: s.n.], 2020. \n",
    "\n",
    "A primeira linha do código abaixo indicar a tarefa a ser executada e o modelo a ser utilizado e a segunda linha aplica o modelo para o texto escolhido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mascarar = pipeline(\"fill-mask\", model=\"neuralmind/bert-base-portuguese-cased\")\n",
    "texto = mascarar(\"Batatinha quando nasce, esparrama pelo [MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jdrM8i7NtGHc",
    "outputId": "bdd34b27-7aea-4fd9-f4eb-13d9fb78cdea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.3925571143627167, 'token': 8105, 'token_str': 'chão', 'sequence': 'Batatinha quando nasce, esparrama pelo chão'}\n",
      "{'score': 0.10256581008434296, 'token': 1831, 'token_str': 'corpo', 'sequence': 'Batatinha quando nasce, esparrama pelo corpo'}\n",
      "{'score': 0.05736977979540825, 'token': 1147, 'token_str': 'mundo', 'sequence': 'Batatinha quando nasce, esparrama pelo mundo'}\n",
      "{'score': 0.047487251460552216, 'token': 388, 'token_str': 'ar', 'sequence': 'Batatinha quando nasce, esparrama pelo ar'}\n",
      "{'score': 0.023149045184254646, 'token': 9169, 'token_str': 'rosto', 'sequence': 'Batatinha quando nasce, esparrama pelo rosto'}\n"
     ]
    }
   ],
   "source": [
    "for x in range(len(texto)):\n",
    "  print(texto[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe nas resposta acima que o maior \"score\" foi para a frase que contém o token \"chão\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarefa 2 - Resumo de textos\n",
    "\n",
    "Para realizar o processo de **resumo de texto** (\"summarization\"), iremos utilizar como exemplo o modelo [facebook/bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn) [2] \n",
    "\n",
    "Utilizaremos o texto que está presente na própria página do modelo. \n",
    "\n",
    "[2] LEWIS, Mike e colab. **BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.** CoRR, v. abs/1910.13461, 2019. Disponível em: <http://arxiv.org/abs/1910.13461>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209,
     "referenced_widgets": [
      "48b565d47c774d0e93b87f98cc0a9f7d",
      "368be86e225147199be400a567e58839",
      "7813a67d9611490ea50e1995513d7ad3",
      "3c67b8b9c17f4880bfe40cd1aacd8157",
      "e8aa9d26ccd843ca9a9c7ed7a483dac3",
      "78b4393924624a1cbd2f3ac0c174bfca",
      "68da64446f694869932e15a6a214d0f3",
      "e4059d9e01a54db9bc3578a8e991eca6",
      "f29891b972334d078513cdc1d50ea41d",
      "a79d91127cce4877930a343c00a143be",
      "98f839f29c344b34b1c7b4e1b367a5fd",
      "3982a4ad8e344a4891cfdd29811aab5c",
      "7defb7ff30894593a4f83c7525198afd",
      "63bef8a224be4fad98f5f50659a5fa82",
      "6982d5ec9c28403a9521a37d0245174d",
      "20b304c2e43b42139e2186f5e554065e",
      "2b586505039545fe906a8a0c80d26945",
      "47b3c80b2c2c48419d57981b22224efc",
      "005ffdd8acc24652a7d74f91f3810a54",
      "b3318fabc0ee48df9ba838c7daa0bf97",
      "35a038e94c7642dda8bb7ab19342ff59",
      "e692528a0c954262bba06ff3c57ccfdc",
      "40084cad24924a51b47fde85ca0cfe03",
      "998d241a12464ce8baf0732b50a4e6c1",
      "c34f3fa8247f42d9b0a5aadcf5d3112d",
      "40f94691d99e4f60a14a15ac392811c1",
      "142e46d9650b42188d611442527cc2f3",
      "72434dd509964a019506db46281ead32",
      "08e33af0824c4e81b4315f3046fb568f",
      "4137d7540a3a4b82a8262c0f1908ee2e",
      "13d0c0b909884f71a746c727cc7fff69",
      "fd4ee9988c354fcdb22df019fad9cc42",
      "7efe62d0fb464201a36b8aa948adc396",
      "7476a72c6afc4bbd87d6a5d2da0961e7",
      "615dc64ba42d4e7b938daf581d146492",
      "5c0f562dfcaf43a88ba0057c69fe99b6",
      "0f21bbf746674311913f9c89ac5f5dab",
      "4a5c1521c1bd48059766ad09fea206f4",
      "eeeaf40311234b71bc06fafa3f9a943b",
      "1e283194faf84febbed20d1768e403e9",
      "3c057b0ce0394a03ab507efc63192da4",
      "d1cfb36fe4224780ad3833d8b3a3d6c1",
      "1faff1a0a3584b9fbc7cb2a7273b4427",
      "59f61c68c22a4c7c9618087babfffa6f",
      "b21fe8063de145c192a2bba9b1a91d46",
      "544709b1359a46b98724b6bd7050fc7a",
      "9aa31b33cd63473e983b6582c7090793",
      "1f4f8e6684e642768b77aa3625db696a",
      "db2f7a82dd0b4ac7a417051c6b0dbfae",
      "1e0357bd77484710af661c266ad2788a",
      "285778bcd96b4bc2acbb14bfeacd5bf0",
      "a39e181eee224af2b51fa6c9fc70452a",
      "c40357b53416440f9355e645e9a9ad9b",
      "4c60a10a4f8045298e6f2beec2e1a6e1",
      "cb13690b8a954c82823c45c11cdacdd9",
      "cacc2c7edfea4e4196c612784ed9a9e3",
      "916e4575ad624c448259b08bd4a862a4",
      "0ebcee63763d4f199f6b12c4cc7a9ecb",
      "a6830f4bd8ac4d1c98909836413b879a",
      "52d446b475c24d07808765cc6102a1a3",
      "4744889889f6441f8e59e1a6f2c8a8fb",
      "082c934f38ca4786817577aa76e2ee62",
      "5b59850eb31542268b77aa32b76414b7",
      "38b4212789204c4c80f657289b1fea58",
      "522e6404d62944afbb73cf02461ee99f",
      "6b708ed00b1d412083ac6261f204b3a3"
     ]
    },
    "id": "dr5s_UwqtuQB",
    "outputId": "57665024-7e94-4835-c304-ad71f0d1b928"
   },
   "outputs": [],
   "source": [
    "resumir = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Ma_m9vy8uOXA"
   },
   "outputs": [],
   "source": [
    "texto = \"\"\"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey \n",
    "building, and the tallest structure in Paris. Its base is square, measuring 125 metres \n",
    "(410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington \n",
    "Monument to become the tallest man-made structure in the world, a title it held for \n",
    "41 years until the Chrysler Building in New York City was finished in 1930. \n",
    "It was the first structure to reach a height of 300 metres. Due to the addition of a \n",
    "broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler \n",
    "Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the \n",
    "second tallest free-standing structure in France after the Millau Viaduct.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ckItVyvuufnO"
   },
   "outputs": [],
   "source": [
    "resumo = resumir(texto, max_length = 40, min_length = 10, do_sample=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zzgue3IUvEJ5",
    "outputId": "b8290ce3-57f2-44a4-e4f5-3a7807f00c29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft'}]\n"
     ]
    }
   ],
   "source": [
    "print(resumo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCbUU_6ssCK5"
   },
   "source": [
    "## Conclusão\n",
    "\n",
    "Este post apresentou como testar modelos de aprendizado de máquinas utilizando a biblioteca Transformers do Hugging Face. \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
