<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ollama | Leonardo Grando</title>
    <link>https://lgrando1.github.io/tag/ollama/</link>
      <atom:link href="https://lgrando1.github.io/tag/ollama/index.xml" rel="self" type="application/rss+xml" />
    <description>Ollama</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 19 Oct 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://lgrando1.github.io/media/icon_hu833f70911ce8d7c0b3dbb80c9eadb7d3_197124_512x512_fill_lanczos_center_3.png</url>
      <title>Ollama</title>
      <link>https://lgrando1.github.io/tag/ollama/</link>
    </image>
    
    <item>
      <title>RAG Offline: Usando LM Studio e Ollama para Processar Documentos</title>
      <link>https://lgrando1.github.io/post/rag/</link>
      <pubDate>Sat, 19 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://lgrando1.github.io/post/rag/</guid>
      <description>&lt;p&gt;Outros posts sobre o tema:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://lgrando1.github.io/post/hface/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Como Criar um Pipeline em Python para Testar Modelos no Hugging Face&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lgrando1.github.io/post/prompt1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dicas de Engenharia de Prompt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lgrando1.github.io/post/ollama/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parte 1 - Instalando o Ollama no Linux&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lgrando1.github.io/post/ollamawin/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parte 2 - Instalando o Ollama no Windows&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lgrando1.github.io/post/llmandroid/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parte 3 - Instalando o Ollama no Android pt.1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lgrando1.github.io/post/llmtermux/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parte 4 - Instalando o Ollama no Android pt.2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lgrando1.github.io/post/waysllms/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parte 5 - 4 Formas de se Utilizar LLMs Offlines em seu Computador&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;Nesta postagem, apresento duas formas de realizar RAG em documentos de forma offline, utilizando o &lt;a href=&#34;https://lmstudio.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LM Studio&lt;/a&gt; e o &lt;a href=&#34;https://ollama.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ollama&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/use-cases/retrieval-augmented-generation?hl=pt-BR&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Geração Aumentada de Recuperação (RAG)&lt;/a&gt; é uma técnica que combina modelos de linguagem com sistemas de busca, recuperando informações relevantes de documentos para melhorar respostas em tarefas de geração de texto.&lt;/p&gt;
&lt;p&gt;Neste vídeo, demonstro como executar o RAG no LM Studio:&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/uO-c7f88x7g&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;Quando gravei o vídeo acima, eu ainda não sabia como realizar o processo de RAG com o Ollama. Graças à excelente extensão &lt;a href=&#34;https://chromewebstore.google.com/detail/page-assist-a-web-ui-for/jfgfiigpkhlkbnfnbobbkinehhfdhndo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Page Assist&lt;/a&gt;, descobri que é possível fazê-lo também no Ollama, como apresentado aqui:&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/RORi14qT0S8&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;Para este vídeo, utilizei um notebook Acer Nitro com CPU Core i5 9300H, 16 GB de RAM e GPU Nvidia GeForce GTX 1650.&lt;/p&gt;
&lt;h3 id=&#34;antes-de-mais-nada-alguns-lembretes&#34;&gt;Antes de mais nada, alguns lembretes:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Lembre-se de que, por rodarem offline, essas aplicações dependem do poder computacional do seu computador, da quantidade de memória RAM e da capacidade de processamento da sua GPU.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Importante:&lt;/strong&gt; Nunca use LLMs como oráculos ou fontes de informação definitiva; já encontrei vários erros em modelos, tanto online quanto offline. Utilize-os apenas como suporte para suas atividades.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Para este post, utilizei um arquivo no formato CSV, importado do Zotero, que contém as publicações acadêmicas das quais sou autor ou coautor. O arquivo pode ser &lt;a href=&#34;https://lgrando1.github.io/uploads/rag.csv&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;encontrado aqui&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Zotero&#34; srcset=&#34;
               /post/rag/00_huc7f1fa9e7bc0d67d0a60773628de907a_70010_c8dd7f5fa8dee415761a72ee79e58fa9.webp 400w,
               /post/rag/00_huc7f1fa9e7bc0d67d0a60773628de907a_70010_b77ae094ee52cd12923eec798f14aa24.webp 760w,
               /post/rag/00_huc7f1fa9e7bc0d67d0a60773628de907a_70010_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/00_huc7f1fa9e7bc0d67d0a60773628de907a_70010_c8dd7f5fa8dee415761a72ee79e58fa9.webp&#34;
               width=&#34;760&#34;
               height=&#34;167&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Lembrando que, em ambos os casos, você vai precisar de um &lt;a href=&#34;https://ollama.com/library?q=embe&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;modelo de embedding&lt;/a&gt;,
Embedding é uma técnica usada em aprendizado de máquina para converter palavras ou frases em vetores numéricos, permitindo que modelos de linguagem compreendam e processem o significado semântico das palavras em um espaço matemático.
Para saber mais, clique &lt;a href=&#34;https://www.elastic.co/pt/what-is/word-embedding&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;aqui&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;1-lm-studio&#34;&gt;1. LM Studio&lt;/h2&gt;
&lt;p&gt;Ainda não havia testado esta ferramenta, mas achei o &lt;a href=&#34;https://lmstudio.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LM Studio&lt;/a&gt; muito prático. A grande vantagem é que não precisa de instalação no Linux (não testei em outros sistemas operacionais). Você pode conhecer os modelos disponíveis neste &lt;a href=&#34;https://lmstudio.ai/models&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Porém, seu uso se mostrou mais complicado do que o Ollama, e não tive sucesso em rodar modelos com mais de 3 bilhões de parâmetros na minha máquina.&lt;/p&gt;
&lt;p&gt;O mais interessante é que ele tem uma interface bem intuitiva:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interface&#34; srcset=&#34;
               /post/rag/01_hu564eec94203fad91b26d2efe172f4509_68404_5baee6e73ad95733c17f9f82f9045ad8.webp 400w,
               /post/rag/01_hu564eec94203fad91b26d2efe172f4509_68404_e41f28eea30a47eb0ddaad7c14434141.webp 760w,
               /post/rag/01_hu564eec94203fad91b26d2efe172f4509_68404_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/01_hu564eec94203fad91b26d2efe172f4509_68404_5baee6e73ad95733c17f9f82f9045ad8.webp&#34;
               width=&#34;760&#34;
               height=&#34;418&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Ele facilita o processo de controle dos modelos LLMs baixados:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interface&#34; srcset=&#34;
               /post/rag/02_hu1c659ce23c562a8617a229eb07a7ea3e_48454_c176a4a08b048212d370accd7f8d7c89.webp 400w,
               /post/rag/02_hu1c659ce23c562a8617a229eb07a7ea3e_48454_7fbdec137466fe05cbae5263af7ebbf2.webp 760w,
               /post/rag/02_hu1c659ce23c562a8617a229eb07a7ea3e_48454_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/02_hu1c659ce23c562a8617a229eb07a7ea3e_48454_c176a4a08b048212d370accd7f8d7c89.webp&#34;
               width=&#34;723&#34;
               height=&#34;333&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;11---adicionando-um-arquivo-para-rag&#34;&gt;1.1 - Adicionando um arquivo para RAG&lt;/h3&gt;
&lt;p&gt;Para incluir um arquivo e realizar o processo de RAG, basta ir à página principal do LM Studio, clicar no ícone de anexo (1) e selecionar o arquivo:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interface&#34; srcset=&#34;
               /post/rag/03_hu1a43ba14c4202302410f2b3a6fe2a728_103587_8230f1b655179b1b59e45eae185b2122.webp 400w,
               /post/rag/03_hu1a43ba14c4202302410f2b3a6fe2a728_103587_7c52d375118e6da70e060a4660228cc1.webp 760w,
               /post/rag/03_hu1a43ba14c4202302410f2b3a6fe2a728_103587_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/03_hu1a43ba14c4202302410f2b3a6fe2a728_103587_8230f1b655179b1b59e45eae185b2122.webp&#34;
               width=&#34;760&#34;
               height=&#34;418&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interface&#34; srcset=&#34;
               /post/rag/04_hu0f0b8d936285567395fc4dbe9d38062c_27857_4221254cfcdcd2933658b36d26711203.webp 400w,
               /post/rag/04_hu0f0b8d936285567395fc4dbe9d38062c_27857_ad28e43779093e22649f22114cef8f32.webp 760w,
               /post/rag/04_hu0f0b8d936285567395fc4dbe9d38062c_27857_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/04_hu0f0b8d936285567395fc4dbe9d38062c_27857_4221254cfcdcd2933658b36d26711203.webp&#34;
               width=&#34;760&#34;
               height=&#34;167&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;12---começando-a-conversar-sobre-o-arquivo&#34;&gt;1.2 - Começando a &amp;ldquo;conversar&amp;rdquo; sobre o arquivo&lt;/h3&gt;
&lt;p&gt;Depois, é só começar a interagir com o arquivo:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interface&#34; srcset=&#34;
               /post/rag/05_hu10588a53c14e5f2e06449b4d08755926_123244_c69eff81209a66ca15b2c6edb9a73732.webp 400w,
               /post/rag/05_hu10588a53c14e5f2e06449b4d08755926_123244_0f03ac7c2dabca341b6865b2a93f861d.webp 760w,
               /post/rag/05_hu10588a53c14e5f2e06449b4d08755926_123244_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/05_hu10588a53c14e5f2e06449b4d08755926_123244_c69eff81209a66ca15b2c6edb9a73732.webp&#34;
               width=&#34;760&#34;
               height=&#34;408&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;No LM Studio, estou utilizando o modelo Llama3.2:3B (não consegui rodar modelos maiores). Note que ele pode se perder ou gerar respostas alucinadas.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interface&#34; srcset=&#34;
               /post/rag/06_hucd5b97ab87e39cc95d6e2329c5314b8f_105735_668cdfff4ec97b797dcedf0afeccef82.webp 400w,
               /post/rag/06_hucd5b97ab87e39cc95d6e2329c5314b8f_105735_785eb96b5a0cd00669c9f854b52b741c.webp 760w,
               /post/rag/06_hucd5b97ab87e39cc95d6e2329c5314b8f_105735_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/06_hucd5b97ab87e39cc95d6e2329c5314b8f_105735_668cdfff4ec97b797dcedf0afeccef82.webp&#34;
               width=&#34;760&#34;
               height=&#34;468&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;2-ollama&#34;&gt;2. Ollama&lt;/h2&gt;
&lt;p&gt;Para realizar RAG no Ollama, é possível utilizar a extensão Page Assist e, após alguns ajustes, começar a realizar inferências.&lt;/p&gt;
&lt;h3 id=&#34;21---configurando-a-extensão-page-assist&#34;&gt;2.1 - Configurando a extensão Page Assist&lt;/h3&gt;
&lt;p&gt;Com o servidor do Ollama rodando, basta abrir a extensão em seu navegador:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interface&#34; srcset=&#34;
               /post/rag/07_hu16e36e91de1ec6639c32d7d61c837f79_50206_b937ece7038de9cee312a7d2c421d9d7.webp 400w,
               /post/rag/07_hu16e36e91de1ec6639c32d7d61c837f79_50206_b99a727fe8d04525b3e9131f6c04479e.webp 760w,
               /post/rag/07_hu16e36e91de1ec6639c32d7d61c837f79_50206_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/07_hu16e36e91de1ec6639c32d7d61c837f79_50206_b937ece7038de9cee312a7d2c421d9d7.webp&#34;
               width=&#34;760&#34;
               height=&#34;417&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Clique no ícone de ajustes (1), vá até o campo &amp;ldquo;RAG Settings&amp;rdquo; (2) e escolha o modelo de embedding (3). Lembre-se de que você já deve ter baixado esse modelo; caso contrário, ele não aparecerá.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interface&#34; srcset=&#34;
               /post/rag/08_hue94a2b13214655294b776e41682e7f28_92117_e3788f6ea4479a46b7f26f24facd9a07.webp 400w,
               /post/rag/08_hue94a2b13214655294b776e41682e7f28_92117_14ac1a5896dc41c614e8246e4a41f01f.webp 760w,
               /post/rag/08_hue94a2b13214655294b776e41682e7f28_92117_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/08_hue94a2b13214655294b776e41682e7f28_92117_e3788f6ea4479a46b7f26f24facd9a07.webp&#34;
               width=&#34;760&#34;
               height=&#34;382&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Caso você ainda não tenha um modelo de embedding, é possível baixar, por exemplo, o modelo &lt;a href=&#34;https://ollama.com/library/nomic-embed-text&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nomic-Embed-text&lt;/a&gt; com o comando no terminal:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama pull nomic-embed-text
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Ou diretamente pela extensão:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interface&#34; srcset=&#34;
               /post/rag/21_hu1699c2a26d25fa5cf1b7a4e4f8860ace_129961_d81be1c14d7136b545df86e3e8dfef64.webp 400w,
               /post/rag/21_hu1699c2a26d25fa5cf1b7a4e4f8860ace_129961_39a1b35472b0ecc39c82f3f95216aa0e.webp 760w,
               /post/rag/21_hu1699c2a26d25fa5cf1b7a4e4f8860ace_129961_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/21_hu1699c2a26d25fa5cf1b7a4e4f8860ace_129961_d81be1c14d7136b545df86e3e8dfef64.webp&#34;
               width=&#34;760&#34;
               height=&#34;395&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interface&#34; srcset=&#34;
               /post/rag/22_hu026ecbdd3e83e3d260ab941f9b6ac285_12289_e5d711b7923b83335cd0333ebc75364b.webp 400w,
               /post/rag/22_hu026ecbdd3e83e3d260ab941f9b6ac285_12289_7582307b2cf62ac402e659c2f942a683.webp 760w,
               /post/rag/22_hu026ecbdd3e83e3d260ab941f9b6ac285_12289_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/22_hu026ecbdd3e83e3d260ab941f9b6ac285_12289_e5d711b7923b83335cd0333ebc75364b.webp&#34;
               width=&#34;625&#34;
               height=&#34;238&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;22---adicionando-um-arquivo-para-rag&#34;&gt;2.2 - Adicionando um arquivo para RAG&lt;/h3&gt;
&lt;p&gt;Agora, podemos incluir o arquivo que desejamos analisar, clicando (1) em &amp;ldquo;Manage Knowledge&amp;rdquo; e adicionando um novo conhecimento no botão &amp;ldquo;Add New Knowledge&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interface&#34; srcset=&#34;
               /post/rag/09_hu6b90390a44dd200cd521ab172b3f66bd_40594_dbaf9af443f27e91e877efb39fd3e62b.webp 400w,
               /post/rag/09_hu6b90390a44dd200cd521ab172b3f66bd_40594_16c939a00536b239b16f7dd1e5e83a86.webp 760w,
               /post/rag/09_hu6b90390a44dd200cd521ab172b3f66bd_40594_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/09_hu6b90390a44dd200cd521ab172b3f66bd_40594_dbaf9af443f27e91e877efb39fd3e62b.webp&#34;
               width=&#34;760&#34;
               height=&#34;338&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;A extensão suporta alguns tipos de arquivos, e você deve nomear esta base de conhecimento:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interface&#34; srcset=&#34;
               /post/rag/10_hu93b46ee30bb3ebee9ccbba945ac88258_17565_b8e51481415650dc9ad6dabf2185dac6.webp 400w,
               /post/rag/10_hu93b46ee30bb3ebee9ccbba945ac88258_17565_c78139653084938b19e0f95eb61cb8c8.webp 760w,
               /post/rag/10_hu93b46ee30bb3ebee9ccbba945ac88258_17565_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/10_hu93b46ee30bb3ebee9ccbba945ac88258_17565_b8e51481415650dc9ad6dabf2185dac6.webp&#34;
               width=&#34;500&#34;
               height=&#34;468&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interface&#34; srcset=&#34;
               /post/rag/11_hu36522c1a2aa8afa170f75eef26d8aaf5_18574_f57c5ac2bb216e93400104d675f05c5e.webp 400w,
               /post/rag/11_hu36522c1a2aa8afa170f75eef26d8aaf5_18574_147fd3ca465ab0e8a8525cdbf1abd3a1.webp 760w,
               /post/rag/11_hu36522c1a2aa8afa170f75eef26d8aaf5_18574_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/11_hu36522c1a2aa8afa170f75eef26d8aaf5_18574_f57c5ac2bb216e93400104d675f05c5e.webp&#34;
               width=&#34;506&#34;
               height=&#34;479&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Aguarde enquanto ele processa o documento (embedding):&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interface&#34; srcset=&#34;
               /post/rag/12_hu7bff6983ed3a367736080dfbefe571b1_21797_a3bb05760c3155354696d97edc274868.webp 400w,
               /post/rag/12_hu7bff6983ed3a367736080dfbefe571b1_21797_327d8f2f0992f1b916b0caa52b4d8f71.webp 760w,
               /post/rag/12_hu7bff6983ed3a367736080dfbefe571b1_21797_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/12_hu7bff6983ed3a367736080dfbefe571b1_21797_a3bb05760c3155354696d97edc274868.webp&#34;
               width=&#34;760&#34;
               height=&#34;288&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Agora, a base de conhecimento está disponível:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interface&#34; srcset=&#34;
               /post/rag/13_hu5d0de4c92ccc28f8520abd411403e206_21159_772837b25bec96991645d0e086b57211.webp 400w,
               /post/rag/13_hu5d0de4c92ccc28f8520abd411403e206_21159_32fdd93ade2e3540501c7b48fdca1001.webp 760w,
               /post/rag/13_hu5d0de4c92ccc28f8520abd411403e206_21159_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/13_hu5d0de4c92ccc28f8520abd411403e206_21159_772837b25bec96991645d0e086b57211.webp&#34;
               width=&#34;760&#34;
               height=&#34;290&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;23---selecionando-a-base-de-conhecimento&#34;&gt;2.3 - Selecionando a base de conhecimento&lt;/h3&gt;
&lt;p&gt;Voltando à tela inicial da extensão, você verá um novo ícone onde digita as mensagens (1). Clicando nele, abrirá as bases de conhecimento (2), e assim você poderá selecionar aquela que deseja utilizar para o RAG.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interface&#34; srcset=&#34;
               /post/rag/13_hu5d0de4c92ccc28f8520abd411403e206_21159_772837b25bec96991645d0e086b57211.webp 400w,
               /post/rag/13_hu5d0de4c92ccc28f8520abd411403e206_21159_32fdd93ade2e3540501c7b48fdca1001.webp 760w,
               /post/rag/13_hu5d0de4c92ccc28f8520abd411403e206_21159_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/13_hu5d0de4c92ccc28f8520abd411403e206_21159_772837b25bec96991645d0e086b57211.webp&#34;
               width=&#34;760&#34;
               height=&#34;290&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Após escolher, a base de conhecimento selecionada aparecerá na parte superior da interface:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interface&#34; srcset=&#34;
               /post/rag/14_hu244b95cb7213a649c072987c79f74e50_27259_0810c62cf8018c40135cd3465bc739b1.webp 400w,
               /post/rag/14_hu244b95cb7213a649c072987c79f74e50_27259_343589177583befee82f2076a389837e.webp 760w,
               /post/rag/14_hu244b95cb7213a649c072987c79f74e50_27259_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/14_hu244b95cb7213a649c072987c79f74e50_27259_0810c62cf8018c40135cd3465bc739b1.webp&#34;
               width=&#34;760&#34;
               height=&#34;521&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;24---interagindo-com-o-modelo-sobre-o-documento&#34;&gt;2.4 - Interagindo com o modelo sobre o documento&lt;/h3&gt;
&lt;p&gt;Agora, é só começar a &amp;ldquo;conversar&amp;rdquo; com o modelo sobre o documento:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interface&#34; srcset=&#34;
               /post/rag/15_hud78af8aaf6a1a0bccf582f00a2191616_32527_9bcccc2f5d85f1f45020719e96772cb3.webp 400w,
               /post/rag/15_hud78af8aaf6a1a0bccf582f00a2191616_32527_7fe3abacec8c14bec90a6ec21cf2e9bb.webp 760w,
               /post/rag/15_hud78af8aaf6a1a0bccf582f00a2191616_32527_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/15_hud78af8aaf6a1a0bccf582f00a2191616_32527_9bcccc2f5d85f1f45020719e96772cb3.webp&#34;
               width=&#34;760&#34;
               height=&#34;544&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interface&#34; srcset=&#34;
               /post/rag/16_hucfb258b2fb5f293282062c7dc7093412_19546_c15a975f218c5a178c9addb7a2368e83.webp 400w,
               /post/rag/16_hucfb258b2fb5f293282062c7dc7093412_19546_7cd38c10afab48b348a1e9f8f0c43897.webp 760w,
               /post/rag/16_hucfb258b2fb5f293282062c7dc7093412_19546_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/16_hucfb258b2fb5f293282062c7dc7093412_19546_c15a975f218c5a178c9addb7a2368e83.webp&#34;
               width=&#34;760&#34;
               height=&#34;622&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interface&#34; srcset=&#34;
               /post/rag/17_huc7f91a891e3d91604c4060956ce15bff_104892_eff0d89a8c45ced3f8a152be4d6dfb16.webp 400w,
               /post/rag/17_huc7f91a891e3d91604c4060956ce15bff_104892_2b6b200a867834336660c5d675c105c1.webp 760w,
               /post/rag/17_huc7f91a891e3d91604c4060956ce15bff_104892_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/17_huc7f91a891e3d91604c4060956ce15bff_104892_eff0d89a8c45ced3f8a152be4d6dfb16.webp&#34;
               width=&#34;760&#34;
               height=&#34;495&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;observações&#34;&gt;Observações&lt;/h3&gt;
&lt;p&gt;Devido à limitação do modelo e do hardware, podem ocorrer alucinações e incorreções:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Aqui, ele mostrou 4 artigos em vez de 5:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interface&#34; srcset=&#34;
               /post/rag/18_hu2e54816ab697c2a460cece9fb2f504c7_103755_c047885cfbee8fdc85eaf191f3d6a46a.webp 400w,
               /post/rag/18_hu2e54816ab697c2a460cece9fb2f504c7_103755_d539e52172d30bb2f8d3a3413d0929db.webp 760w,
               /post/rag/18_hu2e54816ab697c2a460cece9fb2f504c7_103755_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/18_hu2e54816ab697c2a460cece9fb2f504c7_103755_c047885cfbee8fdc85eaf191f3d6a46a.webp&#34;
               width=&#34;581&#34;
               height=&#34;725&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;E aqui, ao utilizar um modelo maior (Llama3.1:8B), ele se perdeu:&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Interface&#34; srcset=&#34;
               /post/rag/20_hu72610c462cc9d8c276997d2374fad453_70894_654c08de1ec67eb57b617889ef70938e.webp 400w,
               /post/rag/20_hu72610c462cc9d8c276997d2374fad453_70894_d5453693d317b2bfa9bfa1389f44da74.webp 760w,
               /post/rag/20_hu72610c462cc9d8c276997d2374fad453_70894_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/rag/20_hu72610c462cc9d8c276997d2374fad453_70894_654c08de1ec67eb57b617889ef70938e.webp&#34;
               width=&#34;556&#34;
               height=&#34;740&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;conclusão&#34;&gt;Conclusão&lt;/h2&gt;
&lt;p&gt;As ferramentas e os modelos utilizados ainda são experimentais e devem ser validados antes de qualquer uso em produção. Para analisar arquivos maiores, são necessários mais recursos computacionais, mas isso não diminui o valor do experimento, que pode ser útil em casos específicos, como a análise de currículos, documentos pequenos ou anotações.&lt;/p&gt;
&lt;p&gt;Sucesso a todos!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Instalação e Uso de LLMs Offline no Windows</title>
      <link>https://lgrando1.github.io/post/ollamawin/</link>
      <pubDate>Sun, 22 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://lgrando1.github.io/post/ollamawin/</guid>
      <description>&lt;p&gt;Outros posts sobre o tema em:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lgrando1.github.io/post/hface/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Como Criar um Pipeline em Python para Testar Modelos no Hugging Face&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lgrando1.github.io/post/prompt1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dicas de Engenharia de Prompt&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lgrando1.github.io/post/ollama/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parte 1 - Instalando o Ollama no Linux&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lgrando1.github.io/post/llmandroid/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parte 3 - Instalando LLMs Off-line no Android - pt.1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lgrando1.github.io/post/llmtermux/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parte 4 - Instalando LLMs Off-line no Android - pt.2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lgrando1.github.io/post/waysllms&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parte 5 - Quatro Maneiras de Usar LLMs Offline no Seu Computador&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lgrando1.github.io/post/rag/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parte 6 - RAG Offline: Usando LM Studio e Ollama para Processar Documentos&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Após o &lt;a href=&#34;https://lgrando1.github.io/post/ollama/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;último post&lt;/a&gt; onde relatei a experiencia de usar o Ollama em num computador com Linux, resolvi extender o teste em uma máquina com Windows 10.
Fiquei interessado em saber como o Ollama iria se comportar em um computador de 2013, um Samsung NP500P4C-AD2BR, provido de um processador Core i7 de terceira geração e sem uma GPU discreta.
As únicas modificações que realizei neste computador foi a inclusão de mais 2 GB de RAM (agora com 6 gigas) e a instalação de um SSD no lugar do HD original.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;infohw&#34; srcset=&#34;
               /post/ollamawin/windownsammy_hubab50e7867f3387d1603636a64fbb609_9611_750d7388bb3053bb6d389cbb94ce95b3.webp 400w,
               /post/ollamawin/windownsammy_hubab50e7867f3387d1603636a64fbb609_9611_f7093f5662e642a4720a78f039210a7e.webp 760w,
               /post/ollamawin/windownsammy_hubab50e7867f3387d1603636a64fbb609_9611_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/ollamawin/windownsammy_hubab50e7867f3387d1603636a64fbb609_9611_750d7388bb3053bb6d389cbb94ce95b3.webp&#34;
               width=&#34;403&#34;
               height=&#34;149&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Lembrando que o &lt;a href=&#34;https://ollama.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ollama&lt;/a&gt; é uma ferramenta que facilita o processo de baixar e rodar os modelos LLMs de código aberto. Ele pode ser instalado no Windows, MacOS e o Linux.&lt;/p&gt;
&lt;p&gt;O processo de instalação foi bem tranquilo, baixei o instalador (são quase 700 megas) e segui o processo de instalação padrão do Windows.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;paginaollama&#34; srcset=&#34;
               /post/ollamawin/windonw_hudf41d5a215322309e276d45b7edc5ac3_48414_206cd234781c85d3080e9573a11daa2c.webp 400w,
               /post/ollamawin/windonw_hudf41d5a215322309e276d45b7edc5ac3_48414_e965494d8c6400752c36c582f5dede80.webp 760w,
               /post/ollamawin/windonw_hudf41d5a215322309e276d45b7edc5ac3_48414_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/ollamawin/windonw_hudf41d5a215322309e276d45b7edc5ac3_48414_206cd234781c85d3080e9573a11daa2c.webp&#34;
               width=&#34;760&#34;
               height=&#34;448&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;download&#34; srcset=&#34;
               /post/ollamawin/windown2_hu6dd2f5dc0f8f2994d004fe0c403ba0f4_60647_0c6ed8d8e881e93e5ff66d3723ac0e50.webp 400w,
               /post/ollamawin/windown2_hu6dd2f5dc0f8f2994d004fe0c403ba0f4_60647_cbe5b7794cae7c3f46cf5bd44847af12.webp 760w,
               /post/ollamawin/windown2_hu6dd2f5dc0f8f2994d004fe0c403ba0f4_60647_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/ollamawin/windown2_hu6dd2f5dc0f8f2994d004fe0c403ba0f4_60647_0c6ed8d8e881e93e5ff66d3723ac0e50.webp&#34;
               width=&#34;760&#34;
               height=&#34;448&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;paginaollama&#34; srcset=&#34;
               /post/ollamawin/windowsinstaler_hudd20e2c191b20dc6ad9bdb2ec974c889_24609_041dc2428515075ae60c0d0df5766fe8.webp 400w,
               /post/ollamawin/windowsinstaler_hudd20e2c191b20dc6ad9bdb2ec974c889_24609_d1908c4a007e758e81cbbcbdfabfb5e9.webp 760w,
               /post/ollamawin/windowsinstaler_hudd20e2c191b20dc6ad9bdb2ec974c889_24609_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/ollamawin/windowsinstaler_hudd20e2c191b20dc6ad9bdb2ec974c889_24609_041dc2428515075ae60c0d0df5766fe8.webp&#34;
               width=&#34;760&#34;
               height=&#34;562&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Após o processo de instalação terminar, vai aparecer o icone do Ollama na sua barra de notificação. Então é só abrir o PowerShell e repetir os mesmos comandos do Linux.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;download&#34; srcset=&#34;
               /post/ollamawin/windowsserver_hu85511738daec8878c3a5e9bf464feb0b_710294_ed632c1a480e14662dcfddf21ada31ef.webp 400w,
               /post/ollamawin/windowsserver_hu85511738daec8878c3a5e9bf464feb0b_710294_631c7f64115207de0a720926c613d707.webp 760w,
               /post/ollamawin/windowsserver_hu85511738daec8878c3a5e9bf464feb0b_710294_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/ollamawin/windowsserver_hu85511738daec8878c3a5e9bf464feb0b_710294_ed632c1a480e14662dcfddf21ada31ef.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Então solicitei para baixar e instalar o modelo &lt;a href=&#34;https://ollama.com/library/phi3.5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Phi3.5 da Microsoft&lt;/a&gt; utilizando o comando&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama run &amp;lt;Nome_da_LLM&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;No caso da Phi3&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama run phi3.5
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;O processo de instalação da LLM foi mais demorado devido à limitação da placa Wi-Fi deste computador, e aqui ele rodando, onde realizei a minha pergunta clássica para verificar os modelos LLMs:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Quantos premios Nobéis o Brasil já ganhou?
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;nobel&#34; srcset=&#34;
               /post/ollamawin/windowsnobel_hu2517b82405507f4283767acef5ab506f_42455_f32a3625dd7ec7812d65afda87abe535.webp 400w,
               /post/ollamawin/windowsnobel_hu2517b82405507f4283767acef5ab506f_42455_644445c5d8c6091cce26bda5a34344ed.webp 760w,
               /post/ollamawin/windowsnobel_hu2517b82405507f4283767acef5ab506f_42455_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/ollamawin/windowsnobel_hu2517b82405507f4283767acef5ab506f_42455_f32a3625dd7ec7812d65afda87abe535.webp&#34;
               width=&#34;628&#34;
               height=&#34;440&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Para testar a diferença de desempenho, solicitei a ele:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Você poderia gerar um código python para ler um arquivo excel em um dataframe pandas?
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;ps: Lembrando que estes modelos possuem uma grande aleatoriedade nas respostas, como poderemos ver abaixo:&lt;/p&gt;
&lt;p&gt;No Notebook com Windows, sem GPU demorou: 284 segundos e na máquina com Linux e GPU demorou 138 segundos. É visível a diferença na formação das frases, mas é importante observar que o mesmo modelo respondeu de forma diferente em ambas as máquinas:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Exemplo da resposta do prompt no Windows&#34; srcset=&#34;
               /post/ollamawin/windprompt_huc6ab9f2eb5c69a8238a6f22c9e88e901_59584_46f72eaeb992f5681aabda5ec9c3af71.webp 400w,
               /post/ollamawin/windprompt_huc6ab9f2eb5c69a8238a6f22c9e88e901_59584_e9451f2fe1f82a32d3238607f5ac5390.webp 760w,
               /post/ollamawin/windprompt_huc6ab9f2eb5c69a8238a6f22c9e88e901_59584_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/ollamawin/windprompt_huc6ab9f2eb5c69a8238a6f22c9e88e901_59584_46f72eaeb992f5681aabda5ec9c3af71.webp&#34;
               width=&#34;645&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;&amp;gt;&amp;gt; Você poderia gerar um código python para ler um arquivo excel em um dataframe pandas?
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Certamente! Abaixo está uma pequena função Python que usa &lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;pandas&lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt; e &lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;openpyxl&lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;, ou
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;seja, a biblioteca de manipulação &lt;span class=&#34;k&#34;&gt;do&lt;/span&gt; Excel padrão no Python. Certifique-se primeiro
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;de ter essas bibliotecas instaladas:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sb&#34;&gt;```&lt;/span&gt;bash
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install pandas openpyxl
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sb&#34;&gt;```&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Aqui está um código que lê um arquivo Excel em um DataFrame Pandas:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;A resposta completa dele:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sb&#34;&gt;```&lt;/span&gt;python
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;import pandas as pd
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;def read_excel_to_dataframe&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;file_path&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# Lendo o arquivo excel usando a função read_excel de pandas com engine&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;openpyxl&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nv&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; pddict.read_excel&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;file_path, &lt;span class=&#34;nv&#34;&gt;engine&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;openpyxl&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; df
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Exemplo de uso:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;arquivo_do_campo&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;caminho/para/seu/arq_de_celulares.xlsx&amp;#39;&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# Substitua pela sua&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;localização &lt;span class=&#34;k&#34;&gt;do&lt;/span&gt; arquivo real
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;dataframe_pandas&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; readdon_excel_to_datafrane&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;fileiio_dave&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;print&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;dataframe_pandas&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sb&#34;&gt;```&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Certifique-se de substituir &lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;caminho/para/seu/arq_de_celulares.xlsx&amp;#39;&lt;/span&gt;&lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt; pela
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;localização real &lt;span class=&#34;k&#34;&gt;do&lt;/span&gt; seu arquivo Excel, ou seja, caminho e nome completo para o seu
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;arquivo no disco rígido.
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Essa função lê diretamente um arquivo de planilha compartilhada &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;com extensão
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;.xlsx&lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; usando a engine &lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;openpyxl&lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;, que é necessária para leituras em Python &lt;span class=&#34;k&#34;&gt;do&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Excel moderno &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;.xlsx&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;. O DataFrame resultante pode então ser manipulado ou analisado
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;conforme desejado.
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Se você tiver apenas arquivos simples com planilhas &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;com extensão &lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;.xls&lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt; e precisar
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;de uma versão mais antiga, poderá usar a engine &lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;xlrd&lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt; em vez disso:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sb&#34;&gt;```&lt;/span&gt;python
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;import pandas as pd
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;def read_excel_to_dataframe&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;file_path&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# Lendo o arquivo excel usando a função read_excel sem especificar um engine (o&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;pandas&amp;#39;&lt;/span&gt; padrão lê com xlrd por padrão para &lt;span class=&#34;s1&#34;&gt;&amp;#39;.xls&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nv&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; pd.read_excel&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;file_path&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; df
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Exemplo de uso:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;arquivo_do_campo&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;caminho/para/seu/arq_de_celulares.xls&amp;#39;&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# Apenas caminho se for&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;um arquivo .xls simples
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;dataframe_pandas&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; readdon_excel_to_datafrane&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;fileiio_dave&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;print&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;dataframe_pandas&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sb&#34;&gt;```&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Lembre-se de que, para ficheiros &lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;.xlsx&lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;, &lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;openpyxl&lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt; é necessário e tanto o pandas
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;quanto openpyxl devem estar instalados em seu ambiente Python.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Resposta do Prompt no computador com Linux.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Exemplo de Prompt Linux&#34; srcset=&#34;
               /post/ollamawin/linprompt_hua1dba9888fe69aa87182346e3e718f0f_92928_f4fa6e246ed9c99a704cfa45bfa6f84f.webp 400w,
               /post/ollamawin/linprompt_hua1dba9888fe69aa87182346e3e718f0f_92928_d9b2de36c2be8018757ba04788d0681c.webp 760w,
               /post/ollamawin/linprompt_hua1dba9888fe69aa87182346e3e718f0f_92928_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/ollamawin/linprompt_hua1dba9888fe69aa87182346e3e718f0f_92928_f4fa6e246ed9c99a704cfa45bfa6f84f.webp&#34;
               width=&#34;760&#34;
               height=&#34;632&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;gt;&amp;gt;&amp;gt; Você poderia gerar um código python para ler um arquivo excel em um dataframe pandas?
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Certamente! Abaixo está um exemplo de como você pode usar &lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;pandas&lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt; juntamente com &lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;openpyxl&lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;, que é uma biblioteca Python para leitura e escrita &lt;span class=&#34;k&#34;&gt;do&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Excel, especialmente adequada para arquivos &lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;.xlsx&lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;. Certifique-se primeiro de ter essas bibliotecas instaladas&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; se não estiverem instaladas, você 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pode adicioná-las usando &lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;pip&lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sb&#34;&gt;```&lt;/span&gt;bash
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install pandas openpyxl
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sb&#34;&gt;```&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Aqui está um script Python simples para ler um arquivo Excel e convertê-lo em um DataFrame Pandas:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sb&#34;&gt;```&lt;/span&gt;python
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;import pandas as pd
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Substitua &amp;#39;seu_arquivo.xlsx&amp;#39; pelo nome do seu arquivo de excel reais ou o caminho até ele se estiver no disco rígido.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;nome_do_arq&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;seu_arquivo.xlsx&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# Lendo um arquivo Excel usando pandas, que reconhece automaticamente a engine como openpyxl para formato .xlsx&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;df&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; pd.read extrado de excel&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;nome_do_arq&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;print&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;df.head&lt;span class=&#34;o&#34;&gt;())&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Imprime as primeiras linhas do DataFrame no console; use df fora-de-caso também&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sb&#34;&gt;```&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Substitua &lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;seu_arquivo.xlsx&amp;#39;&lt;/span&gt;&lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt; pelo caminho absoluto ou nome relativo para seu arquivo Excel, e a variável &lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;df&lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt; contém o conteúdo 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;de sua tabela em um objeto DataFrame Pandas com qualquer forma que você precisar realizar cálculos adicionais ou manipulações.
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Se estiver lidando com muitas linhas/colunas grandes, considere usar &lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;chunksize&lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt; para ler o arquivo por partes:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sb&#34;&gt;```&lt;/span&gt;python
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nv&#34;&gt;chunk_size&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;1000&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; chunk_df in pd.read_excel&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;nome_do_arq, &lt;span class=&#34;nv&#34;&gt;chunksize&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;chunk_size&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# Realiza alguma operação com cada parte do dataframe &amp;#39;chunk&amp;#39; aqui...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    print&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;chunk_df.head&lt;span class=&#34;o&#34;&gt;())&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# Se precisar verificá-lo dentro de seu loop&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;sb&#34;&gt;```&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Lembre-se que esta abordagem requer um arquivo Excel sem cabeçalho dinâmico, pois por padrão &lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt;pd.read_excel&lt;span class=&#34;sb&#34;&gt;`&lt;/span&gt; usará a primeira 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;linha como nomes das colunas&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; se sua tabela &lt;span class=&#34;k&#34;&gt;do&lt;/span&gt; excel &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; estruturada de outra forma &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;por exemplo, cada uma dessas primeiras nove 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;células é um cabeçalho&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;, você precisará lidar com isso manualmente ou especificando explicitamente os rótulos nas linhas.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Preciso validar/verificar ambos os códigos gerados, atualizarei o post com a verificação dos códigos, mas é interessante que é possível rodar um LLM de forma offline em um computador com 11 anos de idade.&lt;/p&gt;
&lt;h2 id=&#34;update-12102024&#34;&gt;UPDATE 12/10/2024&lt;/h2&gt;
&lt;p&gt;Neste vídeo apresento uma forma de copiar/transferir os modelos Offline de um computador para outro. Ou no caso que você não queira baixar novamente os modelos em caso de internet limitada.&lt;/p&gt;
&lt;p&gt;Segundo a &lt;a href=&#34;https://github.com/ollama/ollama/blob/main/docs/faq.md#where-are-models-stored&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentação&lt;/a&gt; no Windows os modelos são armazenados na pasta:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;C:&lt;span class=&#34;se&#34;&gt;\U&lt;/span&gt;sers&lt;span class=&#34;se&#34;&gt;\%&lt;/span&gt;username%&lt;span class=&#34;se&#34;&gt;\.&lt;/span&gt;ollama&lt;span class=&#34;se&#34;&gt;\m&lt;/span&gt;odels
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Neste vídeo:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Testo o Ollama no meu computador.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Copio a pasta &amp;ldquo;models&amp;rdquo; para o meu desktop.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Desinstalo o Ollama.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Demonstro que ele não está funcionando.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reinstalo o Ollama usando o instalador previamente baixado.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Verifico que não há nenhum modelo baixado.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Copio a pasta &amp;ldquo;models&amp;rdquo; para C:\Users%username%.ollama\models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Testo o funcionamento dos modelos.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/XK3YcKShmYQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;ps: Não testei (ainda) em outro sistema operacional para verificar se o procedimento também funciona.&lt;/p&gt;
&lt;p&gt;Sucesso a todos!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Instalação e Uso de LLMs Offline no Linux</title>
      <link>https://lgrando1.github.io/post/ollama/</link>
      <pubDate>Sat, 21 Sep 2024 00:00:00 +0000</pubDate>
      <guid>https://lgrando1.github.io/post/ollama/</guid>
      <description>&lt;p&gt;Outros posts sobre o tema em:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lgrando1.github.io/post/hface/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Como Criar um Pipeline em Python para Testar Modelos no Hugging Face&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lgrando1.github.io/post/prompt1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dicas de Engenharia de Prompt&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lgrando1.github.io/post/ollamawin/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parte 2 - Instalando o Ollama no Windows&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lgrando1.github.io/post/llmandroid/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parte 3 - Instalando LLMs Off-line no Android- pt.1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lgrando1.github.io/post/llmtermux/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parte 4 - Instalando LLMs Off-line no Android - pt.2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lgrando1.github.io/post/waysllms&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parte 5 - Quatro Maneiras de Usar LLMs Offline no Seu Computador&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://lgrando1.github.io/post/rag/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Parte 6 - RAG Offline: Usando LM Studio e Ollama para Processar Documentos&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;O recém &lt;a href=&#34;https://www.nature.com/articles/d41586-024-02998-y&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;artigo da Nature&lt;/strong&gt;&lt;/a&gt; trouxe uma discussão sobre o uso de LLMs locais em vez daquelas que utilizamos de forma online como, por exemplo, o Chat-GPT, Gemini e o CoPilot. A preocupação com aspectos como privacidade e o uso de nossos dados quando utilizando os LLMs de terceiros, sem contar que estas ferramentas necessitam de acesso à internet.
Sites como o &lt;a href=&#34;https://huggingface.co/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hugging Face&lt;/a&gt; permitem testar alguns usos destas ferramentas utilizando uma biblioteca com a linguagem Python, como eu já descrevi em &lt;a href=&#34;https://lgrando1.github.io/post/hface/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;uma postagem anterior.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Eu queria algo mais completo como um assistente virtual local e como sou usuário Linux (uso o Pop!_OS 20.04), encontrei este &lt;a href=&#34;https://itsfoss.com/ollama-setup-linux/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;post muito bem explicado&lt;/a&gt; de como rodar uma LLM de maneira off-line no Linux e então resolvi replicar, e conto esta experiência abaixo.&lt;/p&gt;
&lt;p&gt;O &lt;a href=&#34;https://ollama.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ollama&lt;/a&gt; é uma ferramenta que facilita o processo de baixar e rodar os modelos LLMs de código aberto. Ele pode ser instalado no Windows, MacOS e o Linux. Apenas seguir o &lt;a href=&#34;https://ollama.com/download&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;procedimento de instalação presente no site deles&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;O vídeo abaixo mostra o desempenho destes modelos em meu computador para duas tarefas de geração de códigos em duas linguagens de programação.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/bcDNx7GaXSU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;No meu caso utilizei o comando abaixo, mas recomendo que você siga o procedimento descrito pelo site pois o mesmo pode alterar conforme novas atualizações.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Repetindo: siga o procedimento de instalação conforme descrito no site deles, não este daqui&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;curl -fsSL https://ollama.com/install.sh &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; sh 
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;O código acima irá baixar o Ollama em sua máquina e rodar o script de instalação. Você pode auditar o script de &lt;a href=&#34;https://github.com/ollama/ollama/blob/main/scripts/install.sh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;instalação aqui&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A minha máquina é um notebook Acer Nitro que adquiri no final de 2020. Ele possui um Core i5 9300H, 16 GB de RAM e uma GPU Nvidia GeForce GTX 1650. O que fica interessante, pois o Ollama reconheceu a GPU.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;infohw&#34; srcset=&#34;
               /post/ollama/neofetch_huc57dbd0c962ac1aad17efa80b580855c_61989_daf0f69daa090140162d0bb870490db6.webp 400w,
               /post/ollama/neofetch_huc57dbd0c962ac1aad17efa80b580855c_61989_e0e7a9606a7a95aaa22244ecc7136c1b.webp 760w,
               /post/ollama/neofetch_huc57dbd0c962ac1aad17efa80b580855c_61989_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/ollama/neofetch_huc57dbd0c962ac1aad17efa80b580855c_61989_daf0f69daa090140162d0bb870490db6.webp&#34;
               width=&#34;626&#34;
               height=&#34;532&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Na &lt;a href=&#34;https://itsfoss.com/ollama-setup-linux/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;postagem que usei como referência&lt;/a&gt; para instalar, o autor descreve que o Notebook dele não possui uma GPU discreta, o que influenciou no desempenho. E o modelo escolhido vai também influenciar.&lt;/p&gt;
&lt;p&gt;Hora de testar se o Ollama está rodando, num browser digite:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Testando o Ollama no Browser&#34; srcset=&#34;
               /post/ollama/ollamabrowser_hu80b020927a40b57eb1a0e755609e6b5c_5448_ac09abb31baa59c3a17be38cea8a599d.webp 400w,
               /post/ollama/ollamabrowser_hu80b020927a40b57eb1a0e755609e6b5c_5448_717c5e0842dace38e55630e53f2bd880.webp 760w,
               /post/ollama/ollamabrowser_hu80b020927a40b57eb1a0e755609e6b5c_5448_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/ollama/ollamabrowser_hu80b020927a40b57eb1a0e755609e6b5c_5448_ac09abb31baa59c3a17be38cea8a599d.webp&#34;
               width=&#34;306&#34;
               height=&#34;111&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Aqui mostrou que está funcionando.&lt;/p&gt;
&lt;p&gt;Agora é hora de baixar o modelo LLM. No &lt;a href=&#34;https://ollama.com/library&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;site&lt;/a&gt; existe vários modelos. Já testei o llama3.1. Este &lt;a href=&#34;https://ollama.com/library/llama3.1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;modelo desenvolvido pela Meta&lt;/a&gt; e que possui três níveis de parâmetros 8, 70 e 405 bilhões de parâmetros. Acabei escolhendo o modelo de 8B. São aproximadamente 4.7 GB utilizado de armazenamento. Mas ai fica o critério de cada um. Para este post vou apresentar o processo de instalação do modelo &lt;a href=&#34;https://ollama.com/library/phi3.5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;phi3.5 da Microsoft&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Para dar um &amp;ldquo;pull&amp;rdquo; em um modelo LLM desejado, utiliza-se o comando:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama pull &amp;lt;Nome_da_LLM&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Então para baixar e instalar o modelo &lt;a href=&#34;https://ollama.com/library/phi3.5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Phi3.5 da Microsoft&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama pull phi3.5
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;phi instalado&#34; srcset=&#34;
               /post/ollama/phi3_hu4521b4dd586ea2a9b9782a51cff3b5b0_38614_52705a8b42c9db27ffb2388f5986e5ae.webp 400w,
               /post/ollama/phi3_hu4521b4dd586ea2a9b9782a51cff3b5b0_38614_d91c693f3425617963bb94a2ecab009c.webp 760w,
               /post/ollama/phi3_hu4521b4dd586ea2a9b9782a51cff3b5b0_38614_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/ollama/phi3_hu4521b4dd586ea2a9b9782a51cff3b5b0_38614_52705a8b42c9db27ffb2388f5986e5ae.webp&#34;
               width=&#34;760&#34;
               height=&#34;213&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Agora vamos &lt;strong&gt;listar&lt;/strong&gt; as imagens que estão presentes no seu computador.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama list
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;LLM instaladas&#34; srcset=&#34;
               /post/ollama/ollamalist_hu49f033c139ceae7bbbe46f6319b2a075_18154_2a1604f69fde0f35f72685ae79b87aaa.webp 400w,
               /post/ollama/ollamalist_hu49f033c139ceae7bbbe46f6319b2a075_18154_d6645b5a7868aa6685f360d87c1bfc76.webp 760w,
               /post/ollama/ollamalist_hu49f033c139ceae7bbbe46f6319b2a075_18154_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/ollama/ollamalist_hu49f033c139ceae7bbbe46f6319b2a075_18154_2a1604f69fde0f35f72685ae79b87aaa.webp&#34;
               width=&#34;504&#34;
               height=&#34;182&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Para &lt;strong&gt;rodar&lt;/strong&gt; uma das LLMs com o código:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama run &amp;lt;Nome_da_LLM&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;No caso da Phi3&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama run phi3.5
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Mas antes de tudo, para fins de demostração, garantirei que não está ocorrendo comunicação com a internet:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Desligando o WiFi&#34; srcset=&#34;
               /post/ollama/wifi_hu0518b1696c86dec125af7946952d8ffb_11007_f619ccb51aa24483439161d2913aca46.webp 400w,
               /post/ollama/wifi_hu0518b1696c86dec125af7946952d8ffb_11007_21da799eb1e940cdb3ae4d8e3e025a8f.webp 760w,
               /post/ollama/wifi_hu0518b1696c86dec125af7946952d8ffb_11007_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/ollama/wifi_hu0518b1696c86dec125af7946952d8ffb_11007_f619ccb51aa24483439161d2913aca46.webp&#34;
               width=&#34;736&#34;
               height=&#34;255&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Aqui vou pedir para que ele me gere um código Python para conectar a uma base do MySQL:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Exemplo de Prompt&#34; srcset=&#34;
               /post/ollama/exemplophi_hua192235b4453e72befac9f1c824da6f4_121326_8a5d45d3857871cd2450e252e9b3b157.webp 400w,
               /post/ollama/exemplophi_hua192235b4453e72befac9f1c824da6f4_121326_8caf1b0bbf85d5d4893be3aced9650a2.webp 760w,
               /post/ollama/exemplophi_hua192235b4453e72befac9f1c824da6f4_121326_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/ollama/exemplophi_hua192235b4453e72befac9f1c824da6f4_121326_8a5d45d3857871cd2450e252e9b3b157.webp&#34;
               width=&#34;760&#34;
               height=&#34;708&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Não vou me estender na utilização dele ou de outro modelo, mas é possível utilizar o próprio terminal para conversar com a LLM, e existem formas de conversar via interface gráfica, o que fica para um próximo post.&lt;/p&gt;
&lt;p&gt;Agora para avaliar o uso computacional da minha máquina, vou utilizando o utilitário Nvidia-smi em que é possível ver o quanto ele está utilizando os recursos da GPU&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Uso GPU&#34; srcset=&#34;
               /post/ollama/nvidia_hubc3997401dc88a6996c26f54a459afaf_41310_98f0f646c268241808fa529e6edb22a6.webp 400w,
               /post/ollama/nvidia_hubc3997401dc88a6996c26f54a459afaf_41310_20e94217650cb76e36823c727e3031ae.webp 760w,
               /post/ollama/nvidia_hubc3997401dc88a6996c26f54a459afaf_41310_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/ollama/nvidia_hubc3997401dc88a6996c26f54a459afaf_41310_98f0f646c268241808fa529e6edb22a6.webp&#34;
               width=&#34;584&#34;
               height=&#34;497&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;E em relação ao uso computacional da CPU e do consumo de memória RAM ele não ficou utilizavel, lembrando que o Phi3.5 é um modelo particularmente pequeno. O print abaixo apresenta o consumo computacional durante uma inferência:&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Durante Inferência&#34; srcset=&#34;
               /post/ollama/inferencia_hu3b6af082a6dbe67da50599503b38f4b2_257880_8d493d241e88bed9925f02aac390d399.webp 400w,
               /post/ollama/inferencia_hu3b6af082a6dbe67da50599503b38f4b2_257880_70f92590570ac49bb4f09ae10b0d960f.webp 760w,
               /post/ollama/inferencia_hu3b6af082a6dbe67da50599503b38f4b2_257880_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/ollama/inferencia_hu3b6af082a6dbe67da50599503b38f4b2_257880_8d493d241e88bed9925f02aac390d399.webp&#34;
               width=&#34;760&#34;
               height=&#34;426&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Agora para &lt;em&gt;sair&lt;/em&gt; do Ollama, basta digitar no prompt:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;/bye
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;bye&#34; srcset=&#34;
               /post/ollama/bye_huea65a25b44a75455c77df55c2e60a2dd_4744_14ba64c8fba48d1dd9817ed23edb5451.webp 400w,
               /post/ollama/bye_huea65a25b44a75455c77df55c2e60a2dd_4744_aae2e8326aec6bfc92bb95f4ee43eb92.webp 760w,
               /post/ollama/bye_huea65a25b44a75455c77df55c2e60a2dd_4744_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/ollama/bye_huea65a25b44a75455c77df55c2e60a2dd_4744_14ba64c8fba48d1dd9817ed23edb5451.webp&#34;
               width=&#34;229&#34;
               height=&#34;62&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;E para gerenciar e &lt;em&gt;deletar os modelos LLMs&lt;/em&gt;, é possível listar e solicitar a remoção da imagem.
PS: peço desculpas na imagem abaixo por que digitei um comando errado, por isto ocultei o mesmo, para evitar confusão.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;ollama rm &amp;lt;nome_da_LLM&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Deletando um LLM&#34; srcset=&#34;
               /post/ollama/delete_hubafa5203375a5bde6d5718029e372e93_32239_597180a4a0827ea1566da51c787c39a1.webp 400w,
               /post/ollama/delete_hubafa5203375a5bde6d5718029e372e93_32239_0055384a32f8b9e835b6229d359262d6.webp 760w,
               /post/ollama/delete_hubafa5203375a5bde6d5718029e372e93_32239_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://lgrando1.github.io/post/ollama/delete_hubafa5203375a5bde6d5718029e372e93_32239_597180a4a0827ea1566da51c787c39a1.webp&#34;
               width=&#34;477&#34;
               height=&#34;333&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Este tutorial aborda apenas alguns aspectos do uso do Ollama, o &lt;a href=&#34;https://itsfoss.com/ollama-setup-linux/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorial que serviu como base&lt;/a&gt; para este experimento possui mais informações, como utilizar a interface gráfica com Docker e também como desinstalar o Ollama. Assim você tem um assistente local para lhe ajudar em tarefas simples. Ontem testei o uso do Llamma 3.1 para criar um banco de dados no MySQL e para implementar um código Python para interagir com este banco de dados e o código proposto funcionou. Mas é preciso testar mais.&lt;/p&gt;
&lt;p&gt;Sucesso a todos!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
